[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "How to Win Netrunner Quick Draft (Corp)\n\n\n\nnetrunner\n\n\n\nIndian Union Stock Exchange into Indian Union Stock Exchange into Indian Union Stock Exchange into…\n\n\n\n\n\nFeb 2, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I (Probably) Won’t Boost Your Grade\n\n\n\npedagogy\n\nvalues\n\n\n\nAKA Prof. Dee bums you out.\n\n\n\n\n\nDec 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAlbums of the Year: 2025\n\n\n\noff-topic\n\n\n\nRejoice, Rejoice!\n\n\n\n\n\nDec 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPluribus is about AI, right?\n\n\n\nvalues\n\noff-topic\n\n\n\nClickbait-y thoughts on Pluribus\n\n\n\n\n\nNov 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOn Charlie Kirk\n\n\n\npedagogy\n\nvalues\n\n\n\nOn flying a flag at half-staff for Charlie Kirk\n\n\n\n\n\nSep 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExtended Thoughts on AI\n\n\n\npedagogy\n\ncompbio\n\n\n\nFurther thoughts for students and interested educators on AI in the classroom\n\n\n\n\n\nSep 2, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2025\n\nRuttenberg, Dee M., Scott W. Wolf, Andrew E. Webb, Eli S. Wyman, Michelle L. White, Diogo Melo, Ian M. Traniello, and Sarah D. Kocher. “Queen loss unmasks cryptic worker influence and decentralizes the bumble bee social network.” bioRxiv (2025): 2025.01.07.630106\n\n\n\n2024\n\nRuttenberg, Dee M., Simon A. Levin, Ned S. Wingreen, and Sarah D. Kocher. “Variation in season length and development time is sufficient to drive the emergence and coexistence of social and solitary behavioural strategies.” Proceedings of the Royal Society B 291, no. 2032 (2024): rspb.2024.1221\n\n\n\n2023\n\nWolf, Scott W., Dee M. Ruttenberg, Daniel Y. Knapp, Andrew E. Webb, Ian M. Traniello, Grace C. McKenzie‐Smith, Sophie A. Leheny, Joshua W. Shaevitz, and Sarah D. Kocher. “NAPS: Integrating pose estimation and tag‐based tracking.” Methods in Ecology and Evolution 14, no. 10 (2023): 2041-210X.14201\n\n\n\n2021\n\nRuttenberg, Dee M., Nicholas W. VanKuren, Sumitha Nallu, Shen-Horn Yen, Djunijanti Peggie, David J. Lohman and Marcus R. Kronforst. “The evolution and genetics of sexually dimorphic ‘dual’ mimicry in the butterfly Elymnias hypermnestra.” Proceedings of the Royal Society B 288, no. 2020 (2021): rspb.2020.2192"
  },
  {
    "objectID": "values.html",
    "href": "values.html",
    "title": "Values",
    "section": "",
    "text": "“The relationship between teacher and student is…different from what it was in the past. The former is no longer there for the sake of the latter; both are there for the sake of Wissenschaft [Science]”\n–Wilhelm von Humboldt\n\n\nWhether we are studying Anatomy and Physiology, Evolution, or Bioinformatics, we will be addressing a wide range of important questions. Examining such challenging questions is sometimes difficult, but it becomes easier when we work together. I strongly believe students and teachers can most effectively engage with science in an environment where all students bring 100% of themselves to the classroom: their identities, values, motivations, goals, and experiences. In the pursuit of creating a space where this can be achieved, the following values1 inspire the classroom I hope to create:\nHumanity: We approach science as complete individuals with a range of experiences and perspectives. When we enter a classroom, we enter as the product of our lived experiences, communities, values, and identity. These experiences are valued by all members of our classroom community.\nHumility: By its nature, science is rooted in the appreciation of what we don’t know. None of us are perfect, and the classroom is a space where we can make mistakes. On my end, I will do my best to meet my students where they are, provide the help I can give, and demonstrate patience. On your end, I would like you to feel empowered to support each other, and ask for help when needed.\nCommunication: Teaching is rooted in communication and dialogue. I have found, both as a learner and a teacher, that by talking about lecture material with a friend or putting it in writing, the challenge becomes easier. This is why we have classrooms: to communicate! I will be as communicative as possible as a lecturer, but for our classroom to succeed, you also need to communicate. Answer questions, ask questions, and come to office hours to work through your understanding of the topics.\n\n\n\nReducing a person’s accomplishments and knowledge to a letter grade cannot capture their experiences in a classroom. However, I also am providing a service to Stonehill College and the scientific community as a whole, and part of that service is giving you a letter grade. I ask that you respect the grading structure given in the syllabus. In return, I intend to approach grading in the spirit of humility, and am happy to have open dialogue on how to most effectively grade.\n\n\n\nIn my opinion, the resources provided by AI tools such as ChatGPT, while useful, run against the values of my classroom (as AI models do not approach learning in the spirit of humanity, humility, or communication). Please do not use AI tools as teachers or “Cliffsnotes”. It may make your problem “feel” easier, but it won’t lead to lasting knowledge. Please respect the AI policy of any given class. If you believe the tools provided by an AI resource will help you more effectively engage with an assignment or the classroom, contact me and we will talk."
  },
  {
    "objectID": "values.html#footnotes",
    "href": "values.html#footnotes",
    "title": "Values",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis approach to community building has been inspired by (and in some places, paraphrased) the work of Daniela Palmer and Yan Wang (in their case with the aim of developing a lab culture). My perspective on education has been shaped by a wide variety of educators throughout the years, but most directly by the mentorship of Marion Alberty and the writing of Agnes Callard.↩︎"
  },
  {
    "objectID": "posts/2025-12-16-albums/index.html",
    "href": "posts/2025-12-16-albums/index.html",
    "title": "Albums of the Year: 2025",
    "section": "",
    "text": "Music is good, and now that I have a blog, I don’t have to make a notes document with my albums of the year! Welcome to the discourse!"
  },
  {
    "objectID": "posts/2025-12-16-albums/index.html#twenty-songs-i-love-from-other-albums",
    "href": "posts/2025-12-16-albums/index.html#twenty-songs-i-love-from-other-albums",
    "title": "Albums of the Year: 2025",
    "section": "Twenty Songs I Love from Other Albums",
    "text": "Twenty Songs I Love from Other Albums\nAnywhere — Ratboys\nChains & Whips — Clipse. Kenrick Lamar, Pusha T, & Malice\nCherry Hard Candy — Snocaps\nChildlike Things — FKA Twigs\nCopycats — Danny Brown & underscores\nCrystal — Adult Mom\nDancing in the Club — MJ Lenderman & This is Lorelei\nGemini v. Cancer — Mal Blum\nJust Around the Corner — Beach Bunny\nMirror Shades — clipping.\nNo Joy — The Beths\nRed — Jesse Welles\nSally, When then Wine Runs Out — ROLE MODEL\nSomething Cool — saoirse dream\nSuddenly I See — Scott Bradlee’s Post Modern Jukebox\nTape Runs Out — Julien Baker & TORRES\nTownies — Wednesday\nWe’re Outside, Rejoice! – McKinley Dixon\nX-Games Mode — Dim Wizard, Mike Krol, & Ratboys\nThe Yips — Petey USA"
  },
  {
    "objectID": "posts/2025-12-16-albums/index.html#top-ten-albums-of-2025",
    "href": "posts/2025-12-16-albums/index.html#top-ten-albums-of-2025",
    "title": "Albums of the Year: 2025",
    "section": "Top Ten Albums of 2025",
    "text": "Top Ten Albums of 2025"
  },
  {
    "objectID": "posts/2025-09-15-charliekirk/index.html",
    "href": "posts/2025-09-15-charliekirk/index.html",
    "title": "On Charlie Kirk",
    "section": "",
    "text": "On September 10, 2025, Charlie Kirk was shot dead in Orem, Utah. We are all, across any political, ideological, or social lines, weaker for it. Two children are fatherless, and millions are reeling at the increase in political violence in this country. I hope the magnitude of violence here triggers introspection in all of us on the language we use toward each other, and I pray this is an opportunity for us to build bridges between us, rather than walls.\nFrom September 11, 2025 to September 15, the flag at the university I teach at (Stonehill College) has flown at half mast. While Governor Maura Healey has ordered this for state buildings, as a private institution this was a decision of Stonehill’s. I believe that this decision is rooted in an uncritical evaluation of Charlie Kirk’s life, and is antithetical to the mission statement of the college."
  },
  {
    "objectID": "posts/2025-09-15-charliekirk/index.html#i.-at-half-staff",
    "href": "posts/2025-09-15-charliekirk/index.html#i.-at-half-staff",
    "title": "On Charlie Kirk",
    "section": "I. At Half-Staff",
    "text": "I. At Half-Staff\nCharlie Kirk advocated for a country that many of my students would not be welcome in. He advocated for citizen forces to preserve “white demographics” and repeatedly stoked fears and doubt of the ethics and capabilities of non-white Americans. For instance, he uncritically parroted the statistic “by age of 23, half of all Black males have been arrested and not enough of them have been arrested.”(Bouie 2025)1. He was similarly doubtful of the abilities of black leaders, accusing them of “not having the brain processing power to be taken really seriously”(McNamara and McMenamin 2025). As a teacher, I am honored to work with talented students of color who aspire to take on lines of work far beyond my abilities; when I see the flag at half-staff, I think of my students.\nCharlie Kirk advocated for a country that my loved ones would not be welcome in. I am non-binary, and have been blessed with trans people who have had their lives dramatically bettered through gender-affirming medical care. Kirk called for a “Nuremberg-style trial for every gender-affirming clinic doctor” (McNamara and McMenamin 2025), and referred to trans and non-binary folks like me as “a throbbing middle finger to God” (Perry 2025). My community and I have been blessed that Stonehill has provided a space where I can live my truest and best life; when I see the flag at half-staff, I think of my community.\nDespite frequently touring college campuses, Charlie Kirk actively silenced the speech essential to the academic mission. His Mccarthyite Professor Watchlist was a blatant attempt to intimidate and harass professors for their speech (Bouie 2025). It is not surprising that the Watchlist primarily targeted researchers of color and queer researchers, often for the sin of teaching about vaccination, race or that queer people exist. While I don’t agree with every professor on the watchlist, a university depends on the ability of academics to voice their opinion without fear of retaliation. Otherwise, it becomes the echo chamber he claimed to be fighting against. I’ve been grateful that, at Stonehill, I get to work with this broad array of academics across ideological and political lines to answer truly difficult questions; when I see the flag at half-staff, I think of my calling.\nIn my time here, I’ve had an admiration for how earnestly professors and administrators look to Stonehill’s mission statement for guidance. Stonehill values “fostering a culture where differences are affirmed and anchored by a belief in the inherent dignity of each person”, and encourages students to act and lead “with courage to create a more just and compassionate world.” When the folks who chose to fly the flag at half-staff reflect on this decision, I hope they think about what those words mean2."
  },
  {
    "objectID": "posts/2025-09-15-charliekirk/index.html#footnotes",
    "href": "posts/2025-09-15-charliekirk/index.html#footnotes",
    "title": "On Charlie Kirk",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBecause I’m a dork, I did look up this study (Brame et al. 2014). This study was based on a longitudinal study from 1997-2008 which interviews 537 black children. Hard to become convinced of any absolute numbers from that amount of data (Which the authors agree on, focusing instead on relative prevelence of arrest across the lines of sex and race)↩︎\nI am currently the youngest and most vulnerable faculty member at my university, and there is a precedent for folks losing their jobs for carelessly worded speech on this matter. All this compels me to self-examine why I feel a need to speak up at all. But over the last few days, I’ve been chilled to the core by the silence here and everywhere, allowing hagiography and revisionist history to float to the top of the dirt. And frankly, as a scientist, I think truth is the strongest tool we have to better the world and better each other.↩︎"
  },
  {
    "objectID": "posts/2025-12-23-boostinggrades/index.html",
    "href": "posts/2025-12-23-boostinggrades/index.html",
    "title": "Why I (Probably) Won’t Boost Your Grade",
    "section": "",
    "text": "As a new faculty member, I was surprised by how many times I had the same conversation with students regarding a potential grade boost at the end of the semester, to get from a C+ to a B-, or a B+ to an A-, or an A- to an A. While I appreciate these conversations, they often end up taking the same trajectory. While I don’t doubt these conversations will happen in future semesters, I hope by writing this I can ground these conversations in my values and beliefs."
  },
  {
    "objectID": "posts/2025-12-23-boostinggrades/index.html#argument-1-adding-an-assignment",
    "href": "posts/2025-12-23-boostinggrades/index.html#argument-1-adding-an-assignment",
    "title": "Why I (Probably) Won’t Boost Your Grade",
    "section": "Argument 1: Adding an Assignment",
    "text": "Argument 1: Adding an Assignment\n“If you add an extra-credit assignment, that can give students who really want to do a bit better a chance to boost their grade, while students who are happy with their grade can relax.”\nWhile I love extra credit, post-final-exam extra credit assignments don’t work for me. The reason for this is pragmatism and equity. Pragmatically, once the final exam is done, most students want to enjoy their holiday. If I give you an extra credit assignment, I’d have to give this to not just you, but all other students, which I would need to expect them to do over the holiday3. And frankly, I think it is better to enjoy your break, spend time with your family and recharge. In turn, I don’t feel comfortable giving only the students who ask for extra credit an assignment, as I want to make sure all students have an equal opportunity to boost their grades. I try to address this gap by giving extra credit assignments during the semester, but after the semester, it just does not work pragmatically."
  },
  {
    "objectID": "posts/2025-12-23-boostinggrades/index.html#argument-2-nonrepresentative-grades",
    "href": "posts/2025-12-23-boostinggrades/index.html#argument-2-nonrepresentative-grades",
    "title": "Why I (Probably) Won’t Boost Your Grade",
    "section": "Argument 2: Nonrepresentative Grades",
    "text": "Argument 2: Nonrepresentative Grades\n“Grading is arbitrary anyways. If you look at my work, you’ll see I did better on assignments XYZ, which were more relevant, and most of my mistakes were due to silly blunders irrelevant to the learning goals.”\nThis is the argument that gets me the most, because it often is true. I frequently see an excellent student get a less than perfect grade for arbitrary reasons. It is important that those students know how proud I am of their development, but changing the grade doesn’t help that. Grading is a poor metric for student success. However, I consider grading a responsibility to Stonehill and the scientific community; importantly, that responsibility is not to grade perfectly, but to grade fairly. I owe it to my students to grade how I said I would grade, and I owe it to the scientific community that my grading system is as unbiased as possible. I try to grade in the spirit of humility: making sure the scheme is not just fair but encourages student success and doesn’t punish students arbitrarily for small nits. But finding a “perfect grading scheme” is a lifelong process, and not one that is solved by changing your grade post-hoc."
  },
  {
    "objectID": "posts/2025-12-23-boostinggrades/index.html#argument-3-career-goals",
    "href": "posts/2025-12-23-boostinggrades/index.html#argument-3-career-goals",
    "title": "Why I (Probably) Won’t Boost Your Grade",
    "section": "Argument 3: Career Goals",
    "text": "Argument 3: Career Goals\n“As a teacher, you are trying to help us reach our professional goals, and grade X will be better for me than grade Y.”\nI love helping students, and I want to make sure that my class prepares them to succeed at whatever they want to do. However, I believe it will always be more important professionally to focus on the big picture, and not dwell on smaller things. College is a marathon, and resilience is key to not burning out. This is significantly more important than any individual grade in any individual class. So while a better grade may help you win the battle, dwelling on the smaller picture will absolutely make it harder for you to win the war."
  },
  {
    "objectID": "posts/2025-12-23-boostinggrades/index.html#footnotes",
    "href": "posts/2025-12-23-boostinggrades/index.html#footnotes",
    "title": "Why I (Probably) Won’t Boost Your Grade",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think I grade pretty fairly, but there will always be variance when grading as many assignments as I do. I also sometimes will realize through this conversation that a problem this year was a touch too difficult or had a wrong answer which makes sense in the head of a student, which results in everyone getting points back (yay!)↩︎\n…mostly because just 8 years ago I was making the exact same arguments↩︎\nAnd me to grade over the holiday!↩︎"
  },
  {
    "objectID": "labs/lab8-assembly/index.html",
    "href": "labs/lab8-assembly/index.html",
    "title": "BIO331 – Lab 08: Genome Assembly",
    "section": "",
    "text": "Challenge: Assemble a 1 billion piece puzzle where the pieces are all made of only 4 colors and repeated shapes.\n\n\nIn late December of 2019 doctors and scientists in Wuhan, China were tracking down the cause of an unknown respiritory virus. One part of this effort was to sequence RNA from bronchealveolar fluid from one of the patients. Those data are now available for us to work with:\nData from 9 samples sequenced on December 30, 2019\nPublication of the resulting genome\nAll of these samples are raw RNA sequencing data (represented here in the DNA alphabet) that were generated on an Illumina platform instrument resulting in paired end reads of 250 bp in length. This is a common, but tough, assembly scenario.\nLet’s grab some of the data and see what we can do with it: [Don’t try to run this without verifying that you have several hundred GB of available storage space]\nHere we will be working with SRR11092062 and SRR11092063. Each of these have data from two Illumina Hiseq 3000 runs and comprise a total of ~120 million read pairs (82GB of data), BUT consider that ~98% of this is probably human.\n\nmkdir data\n\n#hiseq\ndeclare -a hiseq=( \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L02_127_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L02_127_2.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L04_122_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L04_122_2.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L02_126_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L02_126_2.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L04_121_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L04_121_2.fq.gz\" \\\n)\n\nfor file in \"${hiseq[@]}\"\ndo\n  echo $file \"\\n\"\n  # parse the SRA id number\n  SRA=$(echo $file | cut -d '/' -f5)\n  echo $SRA\n  #parse 1 or 2\n  READSET=$(echo $file | cut -d '_' -f4)\n  LNUM=$(echo $file | cut -d '_' -f2)\n  echo $LNUM\n  echo $READSET\n  # get the data\n  wget -bqc -o log.txt --output-document data/$SRA.$LNUM.$READSET \"$file\"\ndone\n\nls data/*fq.gz | xargs -P 4 -n 1 gunzip {}\n\ncat data/*.1.fq &gt; data/all_reads.R1.fq\ncat data/*.2.fq &gt; data/all_reads.R2.fq\n\n\n\n\nEvaluate the read qualities for this dataset using fastqc. Are there any red flags?\n\nfastqc -t 2 data/all_reads.R1.fq data/all_reads.R2.fq\n\n\n\n\nIn any genome assembly you are going to have a hard time getting clean assembly graphs when you have material from more than one organism. Our data here are “metatranscriptomic” meaning that we have RNA from many organisms (of course some of this RNA is from an RNA genome so not technically transcribed). Much of this is probably from the host (humans), some maybe RNA from microbiome bacteria, and some is hopefully from our virus of interest.\nThe single biggest thing we can do to help our chances of assembly will be to remove the major contaminants. In this case we should remove any reads that map to the human genome. For this we will use the GRCh38 (https://www.ncbi.nlm.nih.gov/genome/guide/human/) reference genome indexed for alignment with bwa.\n[Don’t run this:]\n\n# download the reference FASTA file\nwget --output-document data/human_ref.fna.gz https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz\n\n#unzip the compressed fna.gz to raw fna file\ngunzip data/human_ref.fna.gz\n\n# index with BWA (runs several hours)\nbwa index data/human_ref.fna\n\nNow that we have an index of a genome we can align with bwa and use samtools to filter reads the DON’T align. That is, we want to collect reads that find no match in the human genome because we are looking for a virus in a haystack of human transcripts.\n[DON’T RUN THIS: Est time to align is several hours using 8 cores]\n\n#align with bwa mem\nbwa mem -t 8 data/human_ref.fna data/all_reads.R1.fq data/all_reads.R2.fq &gt; data/all_reads.sam\n\n# samtools: filter reads that DO NOT map to the reference and convert to fastq\n#convert to BAM\nsamtools view -@ 8 -S -b data/all_reads.sam &gt; data/all_reads.bam\n#sort\nsamtools sort -@ 8 -o data/all_reads.sort.bam data/all_reads.bam\n#output unmapped read pairs\nsamtools view -@ 8 -u -f 12 -F 256 data/all_reads.sort.bam &gt; data/unmapped.bam\nsamtools fastq -@ 8 data/unmapped.bam -1 data/unmapped.R1.fq -2 data/unmapped.R2.fq\n\ndu -sh data/unmapped.*.fq\n\nThat yields about 1.2GB of total data (or about 1.4% of what we started with).\n\n\n\nIllumina technical note on short read de novo assembly\nMelbourne Bioinformatics De Novo Assembly tutorial\nWhat is N50\nAll assembly programs work out from related core principles: In short they all look for short overlapping pieces of every read in a dataset and build a graph of these connections. This structure can then be traversed to find the longest pieces of contiguous overlap across many reads. Longer contigs can then collapse parts of the graph until there are no further overlaps.\nThe data structure involved here is known as a de Bruijn graph.\nWe will try three such programs on our raw data here to see if we can find contiguous sequence in these bronchealveolar fluid RNA sequencing samples.\n\n\nUsing velvet consists of a sequence of two commands:\n\nvelveth - analyzes kmers in the reads\nvelvetg - constructs and refines the assembly graph\n\nRun the following and take a coffee break:\n\n#make an output directory\nmkdir dirAssembly\n\n#set environmental parameter that controls velvet parallel processing\n# will run *parts* of the next steps up to 8x faster than single core operation\nexport OMP_NUM_THREADS=8\n\n# run velveth: kmers\n# the number we provide \"31\" gives a kmer value for building our graphs. In practice we would want to try several to optimize this number.\nvelveth dirAssembly 31 -shortPaired -fastq -separate data/unmapped.R1.fq data/unmapped.R2.fq\n\n#run velvetg: assembly \nvelvetg dirAssembly -min_contig_lgth 5000\n\nLook at the contigs file:\n\nsuppressMessages(library(Biostrings)) #quiet down Biostrings!\nlibrary(ggplot2)\ncontigs = readDNAStringSet('dirAssembly/contigs.fa', format='fasta')\nprint(contigs)\nmaxcontig=max(contigs@ranges@width)\nmaxcontigSTR = contigs[which(contigs@ranges@width == maxcontig)]\nas.character(maxcontigSTR)\n\nwidths = data.frame(contigs@ranges@width)\nnames(widths) = 'contig_length'\nggplot(widths) + \n  geom_density(aes(x=contig_length)) +\n  scale_x_log10()\n\nCheck the sequence of our longest contig with web BLAST: https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\n\n\n\nSPAdes is another assembly program. Here by default there are several rounds of assembly trying different kmer lengths ‘-k’. Here there are also a few extra error correction modules. In general SPAdes requires more computation time and resources, but yields a better assembly.\nmetaSPAdes paper\n\nmetaspades.py -t 8 -k 21,33,55,77 -1 data/unmapped.R1.fq -2 data/unmapped.R2.fq -o spades_output\n\nLook at contigs:\n\ncontigs = readDNAStringSet('spades_output/contigs.fasta', format='fasta')\nprint(contigs)\nmaxcontig=max(contigs@ranges@width)\nmaxcontigSTR = contigs[which(contigs@ranges@width == maxcontig)]\nas.character(maxcontigSTR)\n\nwidths = data.frame(contigs@ranges@width)\nnames(widths) = 'contig_length'\nggplot(widths) + \n  geom_histogram(aes(x=contig_length)) + \n  scale_x_log10()\n\n\n\n\n\nTests multiple kmer lengths\nOptimized for cleaning up metagenomes (mixtures)\n\n\n#installation: run ONLY ONCE, but path specific\nwget https://github.com/voutcn/megahit/releases/download/v1.2.9/MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\ntar zvxf MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\nmv MEGAHIT-1.2.9-Linux-x86_64-static MEGAHIT\n\n#run\n./MEGAHIT/bin/megahit -t 8 -1 data/unmapped.R1.fq -2 data/unmapped.R2.fq  -o dirMEGAHIT\n\nLook at contigs:\n\nsee note about fasta headings: https://github.com/voutcn/megahit/issues/54\n\n\ncontigs = readDNAStringSet('dirMEGAHIT/final.contigs.fa', format='fasta')\nprint(contigs)\nmaxcontig=max(contigs@ranges@width)\nmaxcontigSTR = contigs[which(contigs@ranges@width == maxcontig)]\nas.character(maxcontigSTR)\n\nwidths = data.frame(contigs@ranges@width)\nnames(widths) = 'contig_length'\nggplot(widths) + \n  geom_histogram(aes(x=contig_length)) + \n  scale_x_log10()"
  },
  {
    "objectID": "labs/lab8-assembly/index.html#setup",
    "href": "labs/lab8-assembly/index.html#setup",
    "title": "BIO331 – Lab 08: Genome Assembly",
    "section": "",
    "text": "In late December of 2019 doctors and scientists in Wuhan, China were tracking down the cause of an unknown respiritory virus. One part of this effort was to sequence RNA from bronchealveolar fluid from one of the patients. Those data are now available for us to work with:\nData from 9 samples sequenced on December 30, 2019\nPublication of the resulting genome\nAll of these samples are raw RNA sequencing data (represented here in the DNA alphabet) that were generated on an Illumina platform instrument resulting in paired end reads of 250 bp in length. This is a common, but tough, assembly scenario.\nLet’s grab some of the data and see what we can do with it: [Don’t try to run this without verifying that you have several hundred GB of available storage space]\nHere we will be working with SRR11092062 and SRR11092063. Each of these have data from two Illumina Hiseq 3000 runs and comprise a total of ~120 million read pairs (82GB of data), BUT consider that ~98% of this is probably human.\n\nmkdir data\n\n#hiseq\ndeclare -a hiseq=( \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L02_127_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L02_127_2.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L04_122_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092062/v300043428_L04_122_2.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L02_126_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L02_126_2.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L04_121_1.fq.gz\" \\\n\"https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR11092063/v300043428_L04_121_2.fq.gz\" \\\n)\n\nfor file in \"${hiseq[@]}\"\ndo\n  echo $file \"\\n\"\n  # parse the SRA id number\n  SRA=$(echo $file | cut -d '/' -f5)\n  echo $SRA\n  #parse 1 or 2\n  READSET=$(echo $file | cut -d '_' -f4)\n  LNUM=$(echo $file | cut -d '_' -f2)\n  echo $LNUM\n  echo $READSET\n  # get the data\n  wget -bqc -o log.txt --output-document data/$SRA.$LNUM.$READSET \"$file\"\ndone\n\nls data/*fq.gz | xargs -P 4 -n 1 gunzip {}\n\ncat data/*.1.fq &gt; data/all_reads.R1.fq\ncat data/*.2.fq &gt; data/all_reads.R2.fq"
  },
  {
    "objectID": "labs/lab8-assembly/index.html#qc",
    "href": "labs/lab8-assembly/index.html#qc",
    "title": "BIO331 – Lab 08: Genome Assembly",
    "section": "",
    "text": "Evaluate the read qualities for this dataset using fastqc. Are there any red flags?\n\nfastqc -t 2 data/all_reads.R1.fq data/all_reads.R2.fq"
  },
  {
    "objectID": "labs/lab8-assembly/index.html#cleaning-up-our-data",
    "href": "labs/lab8-assembly/index.html#cleaning-up-our-data",
    "title": "BIO331 – Lab 08: Genome Assembly",
    "section": "",
    "text": "In any genome assembly you are going to have a hard time getting clean assembly graphs when you have material from more than one organism. Our data here are “metatranscriptomic” meaning that we have RNA from many organisms (of course some of this RNA is from an RNA genome so not technically transcribed). Much of this is probably from the host (humans), some maybe RNA from microbiome bacteria, and some is hopefully from our virus of interest.\nThe single biggest thing we can do to help our chances of assembly will be to remove the major contaminants. In this case we should remove any reads that map to the human genome. For this we will use the GRCh38 (https://www.ncbi.nlm.nih.gov/genome/guide/human/) reference genome indexed for alignment with bwa.\n[Don’t run this:]\n\n# download the reference FASTA file\nwget --output-document data/human_ref.fna.gz https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.fna.gz\n\n#unzip the compressed fna.gz to raw fna file\ngunzip data/human_ref.fna.gz\n\n# index with BWA (runs several hours)\nbwa index data/human_ref.fna\n\nNow that we have an index of a genome we can align with bwa and use samtools to filter reads the DON’T align. That is, we want to collect reads that find no match in the human genome because we are looking for a virus in a haystack of human transcripts.\n[DON’T RUN THIS: Est time to align is several hours using 8 cores]\n\n#align with bwa mem\nbwa mem -t 8 data/human_ref.fna data/all_reads.R1.fq data/all_reads.R2.fq &gt; data/all_reads.sam\n\n# samtools: filter reads that DO NOT map to the reference and convert to fastq\n#convert to BAM\nsamtools view -@ 8 -S -b data/all_reads.sam &gt; data/all_reads.bam\n#sort\nsamtools sort -@ 8 -o data/all_reads.sort.bam data/all_reads.bam\n#output unmapped read pairs\nsamtools view -@ 8 -u -f 12 -F 256 data/all_reads.sort.bam &gt; data/unmapped.bam\nsamtools fastq -@ 8 data/unmapped.bam -1 data/unmapped.R1.fq -2 data/unmapped.R2.fq\n\ndu -sh data/unmapped.*.fq\n\nThat yields about 1.2GB of total data (or about 1.4% of what we started with)."
  },
  {
    "objectID": "labs/lab8-assembly/index.html#programs-for-de-novo-assembly",
    "href": "labs/lab8-assembly/index.html#programs-for-de-novo-assembly",
    "title": "BIO331 – Lab 08: Genome Assembly",
    "section": "",
    "text": "Illumina technical note on short read de novo assembly\nMelbourne Bioinformatics De Novo Assembly tutorial\nWhat is N50\nAll assembly programs work out from related core principles: In short they all look for short overlapping pieces of every read in a dataset and build a graph of these connections. This structure can then be traversed to find the longest pieces of contiguous overlap across many reads. Longer contigs can then collapse parts of the graph until there are no further overlaps.\nThe data structure involved here is known as a de Bruijn graph.\nWe will try three such programs on our raw data here to see if we can find contiguous sequence in these bronchealveolar fluid RNA sequencing samples.\n\n\nUsing velvet consists of a sequence of two commands:\n\nvelveth - analyzes kmers in the reads\nvelvetg - constructs and refines the assembly graph\n\nRun the following and take a coffee break:\n\n#make an output directory\nmkdir dirAssembly\n\n#set environmental parameter that controls velvet parallel processing\n# will run *parts* of the next steps up to 8x faster than single core operation\nexport OMP_NUM_THREADS=8\n\n# run velveth: kmers\n# the number we provide \"31\" gives a kmer value for building our graphs. In practice we would want to try several to optimize this number.\nvelveth dirAssembly 31 -shortPaired -fastq -separate data/unmapped.R1.fq data/unmapped.R2.fq\n\n#run velvetg: assembly \nvelvetg dirAssembly -min_contig_lgth 5000\n\nLook at the contigs file:\n\nsuppressMessages(library(Biostrings)) #quiet down Biostrings!\nlibrary(ggplot2)\ncontigs = readDNAStringSet('dirAssembly/contigs.fa', format='fasta')\nprint(contigs)\nmaxcontig=max(contigs@ranges@width)\nmaxcontigSTR = contigs[which(contigs@ranges@width == maxcontig)]\nas.character(maxcontigSTR)\n\nwidths = data.frame(contigs@ranges@width)\nnames(widths) = 'contig_length'\nggplot(widths) + \n  geom_density(aes(x=contig_length)) +\n  scale_x_log10()\n\nCheck the sequence of our longest contig with web BLAST: https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\n\n\n\nSPAdes is another assembly program. Here by default there are several rounds of assembly trying different kmer lengths ‘-k’. Here there are also a few extra error correction modules. In general SPAdes requires more computation time and resources, but yields a better assembly.\nmetaSPAdes paper\n\nmetaspades.py -t 8 -k 21,33,55,77 -1 data/unmapped.R1.fq -2 data/unmapped.R2.fq -o spades_output\n\nLook at contigs:\n\ncontigs = readDNAStringSet('spades_output/contigs.fasta', format='fasta')\nprint(contigs)\nmaxcontig=max(contigs@ranges@width)\nmaxcontigSTR = contigs[which(contigs@ranges@width == maxcontig)]\nas.character(maxcontigSTR)\n\nwidths = data.frame(contigs@ranges@width)\nnames(widths) = 'contig_length'\nggplot(widths) + \n  geom_histogram(aes(x=contig_length)) + \n  scale_x_log10()\n\n\n\n\n\nTests multiple kmer lengths\nOptimized for cleaning up metagenomes (mixtures)\n\n\n#installation: run ONLY ONCE, but path specific\nwget https://github.com/voutcn/megahit/releases/download/v1.2.9/MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\ntar zvxf MEGAHIT-1.2.9-Linux-x86_64-static.tar.gz\nmv MEGAHIT-1.2.9-Linux-x86_64-static MEGAHIT\n\n#run\n./MEGAHIT/bin/megahit -t 8 -1 data/unmapped.R1.fq -2 data/unmapped.R2.fq  -o dirMEGAHIT\n\nLook at contigs:\n\nsee note about fasta headings: https://github.com/voutcn/megahit/issues/54\n\n\ncontigs = readDNAStringSet('dirMEGAHIT/final.contigs.fa', format='fasta')\nprint(contigs)\nmaxcontig=max(contigs@ranges@width)\nmaxcontigSTR = contigs[which(contigs@ranges@width == maxcontig)]\nas.character(maxcontigSTR)\n\nwidths = data.frame(contigs@ranges@width)\nnames(widths) = 'contig_length'\nggplot(widths) + \n  geom_histogram(aes(x=contig_length)) + \n  scale_x_log10()"
  },
  {
    "objectID": "labs/lab5-datawrangling/index.html",
    "href": "labs/lab5-datawrangling/index.html",
    "title": "BIO331 – Lab 05: Data Wrangling",
    "section": "",
    "text": "Reminder If you have not installed pak, do so now. You can install it from CRAN with install.packages(\"pak\") Once installed, load it with library(pak)."
  },
  {
    "objectID": "labs/lab5-datawrangling/index.html#github-classroom-assignment",
    "href": "labs/lab5-datawrangling/index.html#github-classroom-assignment",
    "title": "BIO331 – Lab 05: Data Wrangling",
    "section": "",
    "text": "Reminder If you have not installed pak, do so now. You can install it from CRAN with install.packages(\"pak\") Once installed, load it with library(pak)."
  },
  {
    "objectID": "labs/lab5-datawrangling/index.html#tidying-data",
    "href": "labs/lab5-datawrangling/index.html#tidying-data",
    "title": "BIO331 – Lab 05: Data Wrangling",
    "section": "Tidying data",
    "text": "Tidying data\nIn this step, the pivot_longer function is employed to transform the billboard dataset from a wide format to a long format.\n\nnames_to = \"week\": Specifies that the names of the original set of columns (wk1 to wk76) are to be stored in a new column named week.\nvalues_to = \"rank\": Signifies that the values of the original set of columns will be gathered into a new column named rank.\nvalues_drop_na = TRUE: Ensures that any resulting rows containing NA in the rank column are omitted from the billboard2 dataset.\n\n\nbillboard2 &lt;- billboard %&gt;%\n  pivot_longer(\n    wk1:wk76,\n    names_to = \"week\",\n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n\nbillboard2\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;date&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# ℹ 5,297 more rows\n\n\nNext, the mutate function is utilized to create and modify variables within the long-format dataset created in the previous step.\n\nweek = as.integer(gsub(\"wk\", \"\", week)): Converts the week column to integer by removing the “wk” prefix from the values in the week column and then coercing them to integer.\ndate = as.Date(date.entered) + 7 * (week - 1): Calculates a new date column by adding the number of weeks (converted to days) to the date.entered column, allowing tracking of the specific date related to each week”s data.\ndate.entered = NULL: Removes the original date.entered column after the new date column has been created.\n\n\nbillboard3 &lt;- billboard2 %&gt;%\n  mutate(\n    week = as.integer(gsub(\"wk\", \"\", week)),\n    # Adding to dates in R adds days!\n    date = as.Date(date.entered) + 7 * (week - 1),\n    date.entered = NULL\n  )\n\nbillboard3\n\n# A tibble: 5,307 × 5\n   artist  track                    week  rank date      \n   &lt;chr&gt;   &lt;chr&gt;                   &lt;int&gt; &lt;dbl&gt; &lt;date&gt;    \n 1 2 Pac   Baby Don't Cry (Keep...     1    87 2000-02-26\n 2 2 Pac   Baby Don't Cry (Keep...     2    82 2000-03-04\n 3 2 Pac   Baby Don't Cry (Keep...     3    72 2000-03-11\n 4 2 Pac   Baby Don't Cry (Keep...     4    77 2000-03-18\n 5 2 Pac   Baby Don't Cry (Keep...     5    87 2000-03-25\n 6 2 Pac   Baby Don't Cry (Keep...     6    94 2000-04-01\n 7 2 Pac   Baby Don't Cry (Keep...     7    99 2000-04-08\n 8 2Ge+her The Hardest Part Of ...     1    91 2000-09-02\n 9 2Ge+her The Hardest Part Of ...     2    87 2000-09-09\n10 2Ge+her The Hardest Part Of ...     3    92 2000-09-16\n# ℹ 5,297 more rows\n\n\nFinally, the arrange function is applied to organize the dataset based on the artist, track, and week columns. This operation ensures a coherent and ordered display of the dataset, making it more manageable and intuitive for subsequent analysis.\n\nlong_billboard_sorted &lt;- billboard3 %&gt;% arrange(artist, track, week)\n\nglimpse(long_billboard_sorted)\n\nRows: 5,307\nColumns: 5\n$ artist &lt;chr&gt; \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 Pac\", \"2 Pac\", …\n$ track  &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"Baby Don't Cry (Keep...\", \"Baby Don…\n$ week   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11…\n$ rank   &lt;dbl&gt; 87, 82, 72, 77, 87, 94, 99, 91, 87, 92, 81, 70, 68, 67, 66, 57,…\n$ date   &lt;date&gt; 2000-02-26, 2000-03-04, 2000-03-11, 2000-03-18, 2000-03-25, 20…\n\n\nThis example code creates a new song data frame holding unique artist and track combinations from the billboard3 dataframe, and assigns a unique song_id to each row (representing each unique song).\n\nsong &lt;- billboard3 %&gt;% \n  distinct(artist, track) %&gt;%\n  mutate(song_id = row_number())\n\nglimpse(song)\n\nRows: 317\nColumns: 3\n$ artist  &lt;chr&gt; \"2 Pac\", \"2Ge+her\", \"3 Doors Down\", \"3 Doors Down\", \"504 Boyz\"…\n$ track   &lt;chr&gt; \"Baby Don't Cry (Keep...\", \"The Hardest Part Of ...\", \"Krypton…\n$ song_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n\n\nThe song dataframe is then joined with the billboard3 dataframe to create a new dataframe rank that includes the song_id column.\n\nrank &lt;- billboard3 %&gt;%\n  left_join(song, c(\"artist\", \"track\")) %&gt;%\n  select(song_id, date, week, rank)\n\nglimpse(rank)\n\nRows: 5,307\nColumns: 4\n$ song_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ date    &lt;date&gt; 2000-02-26, 2000-03-04, 2000-03-11, 2000-03-18, 2000-03-25, 2…\n$ week    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1…\n$ rank    &lt;dbl&gt; 87, 82, 72, 77, 87, 94, 99, 91, 87, 92, 81, 70, 68, 67, 66, 57…"
  },
  {
    "objectID": "labs/lab5-datawrangling/index.html#joinsmerges",
    "href": "labs/lab5-datawrangling/index.html#joinsmerges",
    "title": "BIO331 – Lab 05: Data Wrangling",
    "section": "Joins/Merges",
    "text": "Joins/Merges\nIn this section, we will join the flights dataset with the weather dataset from the nycflights13 R package to analyze how weather conditions might have affected the flights.\nMake sure you have loaded in flights.csv and weather.csv!\n\nlibrary(nycflights13)  # R has built-in datasets that can be loaded directly from a library\n\n\nInner Join\nIn an inner join, only the rows with matching keys in both data frames are returned. Rows with non-matching keys are excluded from the result. It’s useful when you want to join datasets based on common key columns, and you are only interested in rows with matching keys in both datasets.\n\n# R example\nflights_weather_inner_joined &lt;- inner_join(flights, weather,by=c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\n\n\n\nLeft Join\nA left join returns all rows from the left dataset and the matched rows from the right dataset. If there is no match found in the right dataset, then the result will contain NA. Use a left join when you want to retain all records from the “left” dataset, and add matching records from the “right” dataset where available.\n\n# R example\nflights_weather_left_joined &lt;- left_join(flights, weather, by=c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\n\n\n\nRight Join\nIn a right join, all rows from the right dataset and the matched rows from the left dataset are returned. If there is no match found in the left dataset, then the result will contain NA. It is the opposite of a left join and is used when you want to retain all records from the “right” dataset.\n\n# R example\nflights_weather_right_joined &lt;- right_join(flights, weather, by=c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\n\n\n\nFull Join\nA full join returns all rows when there is a match in either the left or right dataset. If there is no match found in either dataset, then the result will contain NA. It is useful when you want to retain all records from both datasets.\n\n# R example\nflights_weather_full_joined &lt;- full_join(flights, weather, by=c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\nhead(flights_weather_full_joined)\n\n# A tibble: 6 × 29\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 21 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour.x &lt;dttm&gt;, temp &lt;dbl&gt;, dewp &lt;dbl&gt;,\n#   humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour.y &lt;dttm&gt;\n\n\n\n\nUsing Joins to Analyze the Data\nUse the inner joined dataset to calculate the average departure delay for flights with precipitation greater than 0.5.\n\n# R example\naverage_delay_per_condition &lt;- flights_weather_inner_joined %&gt;%\n  group_by(precip &gt; 0.5) %&gt;%\n  summarise(Average_Departure_Delay = mean(dep_delay, na.rm = TRUE))\n\naverage_delay_per_condition\n\n# A tibble: 2 × 2\n  `precip &gt; 0.5` Average_Departure_Delay\n  &lt;lgl&gt;                            &lt;dbl&gt;\n1 FALSE                             12.6\n2 TRUE                              48.8\n\n\n\n\nAnti Join\nAn anti join returns rows from the left dataset where there are no matching keys in the right dataset. It’s useful for identifying records in one dataset that do not have a counterpart in another dataset.\n\n# R example\nflights_weather_anti_joined &lt;- anti_join(flights, weather, by=c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\n\nhead(flights_weather_anti_joined)\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1     1153           1200        -7     1450           1529\n2  2013     1     1     1154           1200        -6     1253           1306\n3  2013     1     1     1155           1200        -5     1517           1510\n4  2013     1     1     1155           1200        -5     1312           1315\n5  2013     1     1     1157           1200        -3     1452           1456\n6  2013     1     1     1158           1200        -2     1256           1300\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "labs/lab5-datawrangling/index.html#exercises",
    "href": "labs/lab5-datawrangling/index.html#exercises",
    "title": "BIO331 – Lab 05: Data Wrangling",
    "section": "Exercises",
    "text": "Exercises\nWe will use the weather and flights datasets from the nycflights13 package (also provided as .csv files for Python users) for the exercises.\nPlease see the nycflights13”s documentation for more information about the datasets.\n\nExercise 1 – Filtering and Summarizing\n\nTask: Filter the flights dataset to include only flights with a delay of more than 12 hours. Group and count this output by origin and sort the result in descending order.\nExpected Output: A data.frame showing the number of flights delayed over 12 hours by airport, ordered from most to least.\n\n\n\nExercise 2 – Filtering and Summarizing\n\nTask: Calculate the average air time and the number of flights departing from JFK and arriving at LAX in the flights data set. Make sure to report this result in hours.\nExpected Output: A data.frame with a single row showing the average air time in hours and the number of flights from JFK to LAX.\n\n\n\nExercise 3 – Wrangling Airport Data\n\nTask: Using the airports dataset, report the frequency of the time zones of destinations in descending order. Additionally, find an example of an airport with a missing time zone and report the name of the airport, explaining how you checked for it.\nExpected Output:\n\nA data.frame listing the time zones by frequency in descending order.\nThe name of at least one airport with a missing time zone and the code used to identify it.\n\n\n\n\nExercise 4: More Wrangling\n\nTask: Identify the top 3 months with the highest average departure delays in the flights dataset. For these months, calculate the average, minimum, and maximum departure delay.\nExpected Output: A data.frame showing the top 3 months along with their respective average, minimum, and maximum departure delay values."
  },
  {
    "objectID": "labs/lab6-dataviz/index.html",
    "href": "labs/lab6-dataviz/index.html",
    "title": "BIO331 – Lab 06: Data Visualization",
    "section": "",
    "text": "Check out the cheat sheet for ggplot2: Data Visualization with ggplot2: CHEAT SHEET\nDeadline is back to one week from the original precept date.\nPlease save each plot as an image (.png or .jpg) and upload them to your GitHub repository (4 images total).\n\n\n# R\nlibrary(pak)\npackages &lt;- c(\"ggplot2\", \"cowplot\", \"ggthemes\", \"patchwork\")\npak::pkg_install(packages)\n\nlapply(packages, require, character.only = TRUE)"
  },
  {
    "objectID": "labs/lab6-dataviz/index.html#github-classroom-assignment",
    "href": "labs/lab6-dataviz/index.html#github-classroom-assignment",
    "title": "BIO331 – Lab 06: Data Visualization",
    "section": "",
    "text": "Check out the cheat sheet for ggplot2: Data Visualization with ggplot2: CHEAT SHEET\nDeadline is back to one week from the original precept date.\nPlease save each plot as an image (.png or .jpg) and upload them to your GitHub repository (4 images total).\n\n\n# R\nlibrary(pak)\npackages &lt;- c(\"ggplot2\", \"cowplot\", \"ggthemes\", \"patchwork\")\npak::pkg_install(packages)\n\nlapply(packages, require, character.only = TRUE)"
  },
  {
    "objectID": "labs/lab6-dataviz/index.html#exercises",
    "href": "labs/lab6-dataviz/index.html#exercises",
    "title": "BIO331 – Lab 06: Data Visualization",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 – Bar Plot Modification\n\nTask:\n\nAdd a title to the plot: “Distribution of Cars by Class”.\nChange the x-axis label to “Type of Car”.\nColor the bars in blue.\nRotate the x-axis labels by 45 degrees.\n\nExpected Output: An updated plot with the above specifications.\n\nInitial Plot: A simple bar plot displaying the number of cars for each class in the mpg dataset.\n\n# R\nggplot(data = mpg, aes(x = class)) +\n  geom_bar()\n\n\n\nExercise 2 – Histogram Modification\n\nTask:\n\nAdd a title to the plot: “Highway Mileage Distribution”.\nChange the x-axis label to “Miles Per Gallon”.\nFill the histogram bars with green but have a black border.\nSet the bin width to 2.\n\nExpected Output: An updated plot with the above specifications.\n\nInitial Plot: A histogram showcasing the distribution of highway miles per gallon (hwy) from the mpg dataset.\n\n# R\nggplot(data = mpg, aes(x = hwy)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\nExercise 3 – Scatter Plot with Facets\n\nTask:\n\nAdd a title: “Engine Displacement vs. Highway MPG”.\nChange the x-axis label to “Engine Size (liters)” and y-axis label to “Highway MPG”.\nColor the points by class and shape them by the type of drive (e.g., 4wd, fwd, rwd).\nAdd a smooth trend line (with standard error or confidence interval) to the plot. Consider adjusting the alpha of the points for clarity.\nFacet the plot by cyl (number of cylinders) in a 2x2 grid format.\n\nExpected Output: An updated plot with the above specifications.\n\nInitial Plot: A scatter plot illustrating the relationship between engine displacement (displ) and highway MPG (hwy).\n\n# R\nggplot(data = mpg, aes(x = displ, y = hwy)) + \n  geom_point()\n\n\n\nExercise 4: Enhanced Boxplots using after_stat() and patchwork\n\nTask:\n\n\nModify plot1:\n\nColor the boxes based on median value of cty using a gradient from light blue (low mpg) to dark blue (high mpg).\nAdd a title: “City MPG by Manufacturer”.\nRotate x-axis labels by 90 degrees and adjust their size for readability.\nApply a theme of your choice from the ggthemes library\n\nModify plot2:\n\nColor the boxes based on median value of hwy using a gradient from light green (low mpg) to dark green (high mpg).\nAdd a title: “Highway MPG by Manufacturer”.\nRotate x-axis labels by 90 degrees and adjust their size for readability.\nApply the same theme as plot1.\n\nCombine the two modified plots side by side using the patchwork library\n\n\nExpected Output: A plot with the above specifications.\n\n\n# R\nplot1 &lt;- ggplot(data = mpg, aes(x = manufacturer, y = cty)) +\n  geom_boxplot()\n\nplot2 &lt;- ggplot(data = mpg, aes(x = manufacturer, y = hwy)) +\n  geom_boxplot()"
  },
  {
    "objectID": "labs/lab1-setup/index.html",
    "href": "labs/lab1-setup/index.html",
    "title": "BIO331 – Lab 01: Prerequisites",
    "section": "",
    "text": "Prerequisites\nThis course focuses on the R and Unix programming languages, with a main focus on R. Git will also be taught and used for the assignments. A good code editor and environment (Vscode or Rstudio) is also necessary.\nYour take-off will be much easier if you can install all these on your computer before the first precept.\n\nR\nHere are instructions to install R on Windows, Mac and Linux. This includes downloading R from CRAN. Linux users might install R directly with their package manager, such as apt.\nPlease check you can run a simplistic “hello, world” script on your Python installation.\n\n\nText editor / IDE\nStandard usage is to develop code in an integrated development environment (IDE), rather than a simple text editor (though some still use old school editors such as neovim and emacs). We advise to install an IDE, as they show complex code highlighting, can run and help debug code, and connects to AI and git tools.\nWe suggest Rstudio, as it is the best for data visualization and analysis in R. See here for installation on all OS.\nOne of the most popular IDEs is VsCode. Vscode has plugins/extensions for many languages, including those for R – you will find it especially useful in the later half of the class, especially if you do final project 1.\n\n\nGithub\nYou should also set up a github account and install git locally. For easier interaction with github repositories, it is also advised to setup a SSH authentification key: follow instructions here."
  },
  {
    "objectID": "labs/lab7-rblast/index.html",
    "href": "labs/lab7-rblast/index.html",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "The rBLAST package provides R code tools that implement access to the commandline BLAST tools in R. Today we will use this package to build a new BLAST database, run BLAST queries, and visualize the results in ggplot.\n\n\nPackages are a complilation of R functions, data, and code in a specific format that are stored in libraries. Upon initial download, R comes with a basic set of packages installed, but others are avaiable to download. These packages are an easy way to share code with others. In this lesson, we will be downloading and installing the taxonomizr and rBLAST packages. Taxonomizr contains functions that work with NCBI accessions and taxonomy. rBLAST is a basic local alignment search tool, searching for query sequences in databases.\n\nlibrary(taxonomizr)\nlibrary(rBLAST)\nlibrary(ggplot2)\n\n\n\n\nFor this exercise our data can be copied to your user directory with:\n\nmkdir rBLAST\n\nmkdir rBLAST/data\n\n\ncd rBLAST\n\ncp /usr/share/data/BIO331/MATK-1RKIM/* ./\n\nHow many lines are there in each file? Which files should we work with for our BLAST classification?’\n\n\n\nThe database we are going to use this time is too large to copy to each user. Instead you will access the files directly on the server.\n\n\n\nIn the code below, we set a DNA sequence file as a variable of the function readDNAStringSet which loads sequences from an input file including fastq and fasta files. The bl variable is an output of the blast function where each compartment corresponds to a blast search. The cl variable is the result of the top hits when the dns is compared to the blast database.\n\n#makeblastdb creates a folder that contains a blast database like below\n#makeblastdb('/usr/share/data/ncbi/nt/nt.fa', dbtype = \"nucl\") #This takes about 1hr\n\n#prepare for a BLAST query\ndna &lt;- readDNAStringSet('barcode11.fastq', format='fastq')\nbl &lt;- blast(db=\"/usr/share/data/ncbi/nt/nt.fa\")\n\n#Run BLAST query\ncl &lt;- predict(bl, dna[1:10])\n\ncl[1:5,]\n#to view first 5 hits\nsummary(cl)\n#shows the top QueryID hits and other summary statistics including percent identity, alignment length and mismatches. \n\n\n\n\nThe next step is to collect information on the sequences that matched one or more of the query sequences. The code below will create a vector that will contain the accession number (a unique identifier given to a biological sequence when it is submitted to the database). This number is contained in the “SubjectID” term returned from BLAST, but we have to parse it out as that field also includes the GI number.\nA loop can be used to separate out the accession numbers from each of the SubjectID terms from the blast search and store those in the vector ‘accid’.\n\naccid = as.character(cl$SubjectID)\n\n\n\n\nThe code below builds a taxonomy (names) database for taxonomizr.\nThis has already been done for you. So proceed to the next section.\n\nlibdir='data'\ndir.create(libdir)\nsetwd(libdir)\ngetNamesAndNodes()\ngetAccession2taxid(types=c('nucl_gb'))\ngetAccession2taxid()\nsystem(\"gunzip *.gz\")\nread.accession2taxid(list.files('.','accession2taxid'),'accessionTaxa.sql')\nprint(paste('taxonomizr database built and located at', getwd(), sep=' '))\n\n#prepareDatabase('accessionTaxa.sql') #run this somewhere else\n\n\n\n\nThe taxonomizr code below looks up the organism names based on the accession IDs. The ‘taxlist’ object at the end contains the taxonomic assignments for every BLAST hit.\nFirst read the taxonomizr data in:\n\ntaxaNodes&lt;-read.nodes.sql(\"/usr/share/data/taxonomizr/nodes.dmp\")\ntaxaNames&lt;-read.names.sql(\"/usr/share/data/taxonomizr/names.dmp\")\n\nThen match the accession IDs to names in the taxonomy database:\n\n#takes accession number and gets the taxonomic ID\nids&lt;-accessionToTaxa(accid, '/usr/share/data/taxonomizr/accessionTaxa.sql')\n#taxlist displays the taxonomic names from each ID #\ntaxlist=getTaxonomy(ids, taxaNodes, taxaNames)\n\n\n\n\nHere we create a summary data table with full list of taxonimic names (cltax). Then ggplot can be used to visualize the output.\n\ncltax=cbind(cl,taxlist) #bind BLAST hits and taxonomy table\ncolnames(cltax)\n#ggplot for top hits or percent identity of each family\nggplot(data=cltax) + \n  geom_boxplot(aes(x=family, y=Perc.Ident)) + \n  theme(axis.text.x = element_text(angle=90)) +\n  ylim(c(85,100))\n#Comparing alignment length for each family \nggplot(data=cltax) + \n  geom_boxplot(aes(x=family, y=Alignment.Length)) + \n  theme(axis.text.x = element_text(angle=90))\n\nSubsetting can be used to select and exclude variables and observations. In this case, we evaluate how many blast hits each family has after subsetting the data to those matches with &gt;95% identity.\n\n#take the taxonomic names that have above a 95% identity and place in new data set to manipulate\nnewdata &lt;- subset(cltax, Perc.Ident &gt;= 95, \n                  select=c(family, Perc.Ident))\n#creates plot of selected dataset comparing family id and percent identity \nggplot(data=newdata) + aes(x = family, y = Perc.Ident) +\n  geom_point(alpha=0.3, color=\"tomato\", position = \"jitter\") +\n  geom_boxplot(alpha=0) + coord_flip()"
  },
  {
    "objectID": "labs/lab7-rblast/index.html#downloading-packages",
    "href": "labs/lab7-rblast/index.html#downloading-packages",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "Packages are a complilation of R functions, data, and code in a specific format that are stored in libraries. Upon initial download, R comes with a basic set of packages installed, but others are avaiable to download. These packages are an easy way to share code with others. In this lesson, we will be downloading and installing the taxonomizr and rBLAST packages. Taxonomizr contains functions that work with NCBI accessions and taxonomy. rBLAST is a basic local alignment search tool, searching for query sequences in databases.\n\nlibrary(taxonomizr)\nlibrary(rBLAST)\nlibrary(ggplot2)"
  },
  {
    "objectID": "labs/lab7-rblast/index.html#get-data",
    "href": "labs/lab7-rblast/index.html#get-data",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "For this exercise our data can be copied to your user directory with:\n\nmkdir rBLAST\n\nmkdir rBLAST/data\n\n\ncd rBLAST\n\ncp /usr/share/data/BIO331/MATK-1RKIM/* ./\n\nHow many lines are there in each file? Which files should we work with for our BLAST classification?’"
  },
  {
    "objectID": "labs/lab7-rblast/index.html#database-ncbi-nt",
    "href": "labs/lab7-rblast/index.html#database-ncbi-nt",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "The database we are going to use this time is too large to copy to each user. Instead you will access the files directly on the server."
  },
  {
    "objectID": "labs/lab7-rblast/index.html#setting-variables",
    "href": "labs/lab7-rblast/index.html#setting-variables",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "In the code below, we set a DNA sequence file as a variable of the function readDNAStringSet which loads sequences from an input file including fastq and fasta files. The bl variable is an output of the blast function where each compartment corresponds to a blast search. The cl variable is the result of the top hits when the dns is compared to the blast database.\n\n#makeblastdb creates a folder that contains a blast database like below\n#makeblastdb('/usr/share/data/ncbi/nt/nt.fa', dbtype = \"nucl\") #This takes about 1hr\n\n#prepare for a BLAST query\ndna &lt;- readDNAStringSet('barcode11.fastq', format='fastq')\nbl &lt;- blast(db=\"/usr/share/data/ncbi/nt/nt.fa\")\n\n#Run BLAST query\ncl &lt;- predict(bl, dna[1:10])\n\ncl[1:5,]\n#to view first 5 hits\nsummary(cl)\n#shows the top QueryID hits and other summary statistics including percent identity, alignment length and mismatches."
  },
  {
    "objectID": "labs/lab7-rblast/index.html#collecting-blast-hits",
    "href": "labs/lab7-rblast/index.html#collecting-blast-hits",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "The next step is to collect information on the sequences that matched one or more of the query sequences. The code below will create a vector that will contain the accession number (a unique identifier given to a biological sequence when it is submitted to the database). This number is contained in the “SubjectID” term returned from BLAST, but we have to parse it out as that field also includes the GI number.\nA loop can be used to separate out the accession numbers from each of the SubjectID terms from the blast search and store those in the vector ‘accid’.\n\naccid = as.character(cl$SubjectID)"
  },
  {
    "objectID": "labs/lab7-rblast/index.html#build-taxonomizr-database",
    "href": "labs/lab7-rblast/index.html#build-taxonomizr-database",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "The code below builds a taxonomy (names) database for taxonomizr.\nThis has already been done for you. So proceed to the next section.\n\nlibdir='data'\ndir.create(libdir)\nsetwd(libdir)\ngetNamesAndNodes()\ngetAccession2taxid(types=c('nucl_gb'))\ngetAccession2taxid()\nsystem(\"gunzip *.gz\")\nread.accession2taxid(list.files('.','accession2taxid'),'accessionTaxa.sql')\nprint(paste('taxonomizr database built and located at', getwd(), sep=' '))\n\n#prepareDatabase('accessionTaxa.sql') #run this somewhere else"
  },
  {
    "objectID": "labs/lab7-rblast/index.html#getting-species-names",
    "href": "labs/lab7-rblast/index.html#getting-species-names",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "The taxonomizr code below looks up the organism names based on the accession IDs. The ‘taxlist’ object at the end contains the taxonomic assignments for every BLAST hit.\nFirst read the taxonomizr data in:\n\ntaxaNodes&lt;-read.nodes.sql(\"/usr/share/data/taxonomizr/nodes.dmp\")\ntaxaNames&lt;-read.names.sql(\"/usr/share/data/taxonomizr/names.dmp\")\n\nThen match the accession IDs to names in the taxonomy database:\n\n#takes accession number and gets the taxonomic ID\nids&lt;-accessionToTaxa(accid, '/usr/share/data/taxonomizr/accessionTaxa.sql')\n#taxlist displays the taxonomic names from each ID #\ntaxlist=getTaxonomy(ids, taxaNodes, taxaNames)"
  },
  {
    "objectID": "labs/lab7-rblast/index.html#visualizing-blast-hits",
    "href": "labs/lab7-rblast/index.html#visualizing-blast-hits",
    "title": "BIO331 – Lab 07: Bioinformatics and BLAST",
    "section": "",
    "text": "Here we create a summary data table with full list of taxonimic names (cltax). Then ggplot can be used to visualize the output.\n\ncltax=cbind(cl,taxlist) #bind BLAST hits and taxonomy table\ncolnames(cltax)\n#ggplot for top hits or percent identity of each family\nggplot(data=cltax) + \n  geom_boxplot(aes(x=family, y=Perc.Ident)) + \n  theme(axis.text.x = element_text(angle=90)) +\n  ylim(c(85,100))\n#Comparing alignment length for each family \nggplot(data=cltax) + \n  geom_boxplot(aes(x=family, y=Alignment.Length)) + \n  theme(axis.text.x = element_text(angle=90))\n\nSubsetting can be used to select and exclude variables and observations. In this case, we evaluate how many blast hits each family has after subsetting the data to those matches with &gt;95% identity.\n\n#take the taxonomic names that have above a 95% identity and place in new data set to manipulate\nnewdata &lt;- subset(cltax, Perc.Ident &gt;= 95, \n                  select=c(family, Perc.Ident))\n#creates plot of selected dataset comparing family id and percent identity \nggplot(data=newdata) + aes(x = family, y = Perc.Ident) +\n  geom_point(alpha=0.3, color=\"tomato\", position = \"jitter\") +\n  geom_boxplot(alpha=0) + coord_flip()"
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html",
    "href": "labs/lab4-stringwrangling/index.html",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "",
    "text": "This week’s assignment can be found here."
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#using-read.csv-from-base-r",
    "href": "labs/lab4-stringwrangling/index.html#using-read.csv-from-base-r",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Using read.csv from Base R",
    "text": "Using read.csv from Base R\n\n# Reading a CSV file using read.csv from base R\ndata_base_r &lt;- read.csv(\"places_in_princeton.csv\")\n\n# Inspecting the first few rows of the data\nhead(data_base_r)\n\n  id                            name                                address\n1  1                     Nassau Hall     1 Nassau Hall, Princeton, NJ 08544\n2  2 Princeton University Art Museum            Elm Dr, Princeton, NJ 08544\n3  3           Albert Einstein House     112 Mercer St, Princeton, NJ 08540\n4  4        Princeton Public Library 65 Witherspoon St, Princeton, NJ 08542\n5  5         McCarter Theatre Center  91 University Pl, Princeton, NJ 08540\n6  6                   Marquand Park      68 Lovers Ln, Princeton, NJ 08540\n                                            comment\n1                 Historical building in Princeton.\n2 A place with a vast and varied collection of art.\n3                      Albert Einstein's residence.\n4                    The hub of community learning.\n5            Famous for its performances and shows.\n6        A peaceful place to walk and enjoy nature."
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#using-read_csv-from-readr",
    "href": "labs/lab4-stringwrangling/index.html#using-read_csv-from-readr",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Using read_csv from readr",
    "text": "Using read_csv from readr\n\npak::pkg_install(\"readr\")\nlibrary(readr)\n\n\n# Reading a CSV file using read_csv from readr package\ndata_readr &lt;- read_csv(\"places_in_princeton.csv\")\n\nRows: 20 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, address, comment\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Inspecting the first few rows of the data\nhead(data_readr)\n\n# A tibble: 6 × 4\n     id name                            address                          comment\n  &lt;dbl&gt; &lt;chr&gt;                           &lt;chr&gt;                            &lt;chr&gt;  \n1     1 Nassau Hall                     1 Nassau Hall, Princeton, NJ 08… Histor…\n2     2 Princeton University Art Museum Elm Dr, Princeton, NJ 08544      A plac…\n3     3 Albert Einstein House           112 Mercer St, Princeton, NJ 08… Albert…\n4     4 Princeton Public Library        65 Witherspoon St, Princeton, N… The hu…\n5     5 McCarter Theatre Center         91 University Pl, Princeton, NJ… Famous…\n6     6 Marquand Park                   68 Lovers Ln, Princeton, NJ 085… A peac…\n\nwrite_delim(data_readr, \"places_in_princeton.tsv\", delim = \"\\t\")"
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#installing-and-loading-the-stringr-package",
    "href": "labs/lab4-stringwrangling/index.html#installing-and-loading-the-stringr-package",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Installing and loading the stringr package",
    "text": "Installing and loading the stringr package\n\npak::pkg_install(\"stringr\")\nlibrary(stringr)"
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#stringr-examples",
    "href": "labs/lab4-stringwrangling/index.html#stringr-examples",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "stringr examples",
    "text": "stringr examples\nExtracting digits from strings can be crucial to isolate specific numerical information such as prices or zip codes.\n\nstrings &lt;- c(\"123 Main St\", \"Price: $200 0\")\ndigits &lt;- str_extract_all(strings, \"\\\\b\\\\d+\\\\b\") # notice how only one backslash is needed for regex in Python\n\ndigits\n\n[[1]]\n[1] \"123\"\n\n[[2]]\n[1] \"200\" \"0\""
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#tidyr-examples",
    "href": "labs/lab4-stringwrangling/index.html#tidyr-examples",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "tidyr examples",
    "text": "tidyr examples\nExampled pulled from: https://r4ds.hadley.nz/regexps#sec-extract-variables\n\npak::pkg_install(\"tidyr\")\nlibrary(tidyr)\n\n\ndf &lt;- tribble(\n  ~str,\n  \"&lt;Sheryl&gt;-F_34\",\n  \"&lt;Kisha&gt;-F_45\",\n  \"&lt;Brandon&gt;-N_33\",\n  \"&lt;Sharon&gt;-F_38\",\n  \"&lt;Penny&gt;-F_58\",\n  \"&lt;Justin&gt;-M_41\",\n  \"&lt;Patricia&gt;-F_84\",\n)\n\n\ndf |&gt;\n  separate_wider_regex(\n    str,\n    patterns = c(\n      \"&lt;\", # Match the literal character '&lt;'.\n      name = \"[A-Za-z]+\", # Match one or more alphabets (upper or lower case) and create a new column 'name' with the matched value.\n      \"&gt;-\", # Match the literal string '&gt;-'.\n      gender = \".\", # Match any single character (except newline) and create a new column 'gender' with the matched value.\n      \"_\", # Match the literal character '_'.\n      age = \"[0-9]+\" # Match one or more digits and create a new column 'age' with the matched value.\n    )\n  )\n\n# A tibble: 7 × 3\n  name     gender age  \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Sheryl   F      34   \n2 Kisha    F      45   \n3 Brandon  N      33   \n4 Sharon   F      38   \n5 Penny    F      58   \n6 Justin   M      41   \n7 Patricia F      84"
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#exercise-1-reading-tsv-files",
    "href": "labs/lab4-stringwrangling/index.html#exercise-1-reading-tsv-files",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Exercise 1: Reading TSV Files",
    "text": "Exercise 1: Reading TSV Files\n\nA TSV file, places_in_princeton.tsv, uses a tab character as a delimiter between values. Your task is to read this file into R using an appropriate reading function."
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#exercise-2.-regex-to-english",
    "href": "labs/lab4-stringwrangling/index.html#exercise-2.-regex-to-english",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Exercise 2. Regex to English",
    "text": "Exercise 2. Regex to English\n\nExplain what this regex patters does: “\\d+[-]\\d+[-]\\d+\\s” in words. What is an example of something it matches? Leave your response as a comment block in your working script or file."
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#exercise-3-extracting-the-parts-of-an-address",
    "href": "labs/lab4-stringwrangling/index.html#exercise-3-extracting-the-parts-of-an-address",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Exercise 3: Extracting the parts of an address",
    "text": "Exercise 3: Extracting the parts of an address\nWith the address column from the data from Exercise 1, we can extract ZIP codes as follows:\n\n# in R\nzip_codes &lt;- str_extract(data_tsv$address, \"\\\\b\\\\d{5}(?:-\\\\d{4})?\\\\b\")\n\n\nNow, extract the street address from the address string and search each comment for the word “Famous” to determine which locations are famous in Princeton. Follow this up by searching for “historic.” Make sure that your search is case-insensitive and captures when historic is a substring as well, like in “historical”."
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#exercise-4-more-string-extraction",
    "href": "labs/lab4-stringwrangling/index.html#exercise-4-more-string-extraction",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Exercise 4: More string extraction",
    "text": "Exercise 4: More string extraction\n\nIdentify locations with a number in their names and extract that number.\nExtract all words starting with ‘a’ or ‘A’ from the comment column."
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#exercise-5-place-names",
    "href": "labs/lab4-stringwrangling/index.html#exercise-5-place-names",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Exercise 5: Place names",
    "text": "Exercise 5: Place names\n\nWhich Princeton place name has the most vowels? What name has the highest proportion of vowels? (Hint: what is the denominator?)"
  },
  {
    "objectID": "labs/lab4-stringwrangling/index.html#exercise-6-extracting-month-day-year-from-a-date-variable",
    "href": "labs/lab4-stringwrangling/index.html#exercise-6-extracting-month-day-year-from-a-date-variable",
    "title": "BIO331 – Lab 04: Data/String Wrangling",
    "section": "Exercise 6: Extracting month, day, year from a date variable",
    "text": "Exercise 6: Extracting month, day, year from a date variable\nCreate a dataframe called flights from nycflights.csv and create the following variable date_strings:\n\ndate_strings &lt;- paste(flights$year, flights$month, flights$day, sep = \"-\")\n\n\nExtract the months, days, and years from your new variable into three separate variables for each date component"
  },
  {
    "objectID": "labs/lab3-controlflow/index.html",
    "href": "labs/lab3-controlflow/index.html",
    "title": "BIO331 – Lab 03: Control flow",
    "section": "",
    "text": "Make sure that you are comfortable with git and have a basic understanding of how to run code in R. If you need a refresher, please see the Lab 2 materials and external resources. Feel free to ask any remaining questions.\nBefore coming to precept, you should have:\n\nAccepted the assignment intro-to-control-flow from GitHub Classrooms.\nA working installation of RStudio or VS Code\nA git repository for the precept. This git repository should be created using the precept assignment link.\nA draft branch in your assignment repository. Do not work in main!"
  },
  {
    "objectID": "labs/lab3-controlflow/index.html#basic-conditionals",
    "href": "labs/lab3-controlflow/index.html#basic-conditionals",
    "title": "BIO331 – Lab 03: Control flow",
    "section": "Basic conditionals",
    "text": "Basic conditionals\nIn R, the basic conditional statements are if, else, and else if.\n\nset.seed(42) # This ensures that the random number generator will produce the same results each time the script is run\nx &lt;- runif(1, 0, 10)\n\nif (x &gt; 5) {\n     print(\"x is greater than 5\")\n\n} else if (x &lt; 5) {\n     print(\"x is less than 5\")\n\n} else {\n     print(\"x is equal to 5\")\n}\n\n[1] \"x is greater than 5\"\n\n\n‘ifelse’ statements can also be used as a single function to identify multiple individuals within an ordered list:\n\nset.seed(42) # This ensures that the random number generator will produce the same results each time the script is run\nx &lt;- runif(10, 0, 10)\n\nx_greater &lt;- ifelse(x &gt; 5, \"Larger than 5\", \"Smaller than 5\")"
  },
  {
    "objectID": "labs/lab3-controlflow/index.html#loops",
    "href": "labs/lab3-controlflow/index.html#loops",
    "title": "BIO331 – Lab 03: Control flow",
    "section": "Loops",
    "text": "Loops\nThere are two primary loops in R - for and while. for loops are used when the number of iterations is known, while while loops are used when the number of iterations is unknown and require an iterator variable.\nFor loop\n\nfor (i in 1:5) {\n     print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nWhile loop\n\ncounter &lt;- 1 # this is the iterator variable\n\nwhile (counter &lt;= 5) {\n     print(counter)\n     counter &lt;- counter + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5"
  },
  {
    "objectID": "labs/lab3-controlflow/index.html#functions",
    "href": "labs/lab3-controlflow/index.html#functions",
    "title": "BIO331 – Lab 03: Control flow",
    "section": "Functions",
    "text": "Functions\nFunctions are blocks of reusable code. Functions are used to perform a specific task and are only run when they are called.\n\ncalculate_sum &lt;- function(a, b) { # a and b are the function arguments\n     return(a + b)\n}\n\ncalculate_sum(3, 4) # this is where we call the function\n\n[1] 7\n\n\n\nScoping\nScoping refers to the visibility of variables. In R, there are two main scopes: global and local. Global variables are accessible throughout the entire script, while local variables are only available within the function or script in which they are defined.\n\nglobal_var &lt;- 10\n\nfunction_with_scope &lt;- function() { \n  local_var &lt;- 5\n  return(global_var + local_var) # function_with_scope() has access to global_var even though it is not defined in the function\n}\n\nfunction_with_scope()\n\n[1] 15\n\n\n\n\nRecursion\nA function that calls itself is known as a recursive function. Recursive function are only used for very specific problems, such as when you need to solve a problem that can be broken down into smaller, similar problems.\n\nfactorial &lt;- function(n) {\n     if (n &lt;= 1) {\n          return(1)\n\n     } else {\n     return(n * factorial(n-1)) # this is where factorial() calls itself\n     }\n}\n\nfactorial(5)\n\n[1] 120"
  },
  {
    "objectID": "labs/lab3-controlflow/index.html#exercises",
    "href": "labs/lab3-controlflow/index.html#exercises",
    "title": "BIO331 – Lab 03: Control flow",
    "section": "Exercises",
    "text": "Exercises\n\n1. Conditionals\nConsider a vector containing the values: 91, 77, 68, 65, 89, 72, 85, 90, 80.\nTip: A vector is a way of storing data, in this case, ordered integers. To access the nth number in a vector, you can use vector[n]\n\ngrades &lt;- c(91, 77, 68, 65, 89, 72, 85, 90, 80)\ngrades[1]\n\n[1] 91\n\n\nWrite a conditional script that:\n\nTags grades above 90 as “Excellent”\nBetween 80 and 90 (inclusive) as “Good”\nBelow 80 as “Fair”\n\nStore the result in a new variable called grade_tags. Ensure the order in grade_tags corresponds to the order of grades in the grades vector, and print the result.\ngrade_tags &lt;- ifelse( grades &gt; 90, “Excellent”, ifelse(grades &gt;= 80, “Good”, “Fair”) )\n\n\n2. For\nCreate a sequence of numbers from 3 to 15 (included). Using a for loop, compute the cumulative product of these numbers and print the results.\n\n\n3. While\nStarting with a number 50, decrease it by 5% in every iteration using a while loop. Continue the iterations until the number goes below 20. Return and print the number of iterations it took.\n\n\n4. Functions with conditionals and loops\nWrite a function named series_sum that accepts two arguments: a start value ‘s’ and an end value ‘e’. The function should sum all numbers from ‘s’ to ‘e’. However, if a number in the series is divisible by 3, it should be skipped.\nTip: The modulo operator (%% in R) returns the remainder of the division of two numbers.\n\n# as in R\n5%%2\n\n[1] 1\n\n\n\n\n5. Recursive Functions\nThe Lucas series is a sequence of numbers similar to the Fibonacci series but starts with 2 and 1 instead of 0 and 1. Write a recursive function lucas that computes the nth value of the Lucas series."
  },
  {
    "objectID": "labs/lab2-git/index.html",
    "href": "labs/lab2-git/index.html",
    "title": "BIO331 – Lab 02: Intro to git",
    "section": "",
    "text": "Labs can be found here.\nWe’re going to mostly be doing the precept problem sets during class, but I’ll be posting them as well. If you don’t finish during class, you can finish them on your own time and submit them to the appropriate assignment in 1 week to GitHub Classroom."
  },
  {
    "objectID": "labs/lab2-git/index.html#github",
    "href": "labs/lab2-git/index.html#github",
    "title": "BIO331 – Lab 02: Intro to git",
    "section": "GitHub",
    "text": "GitHub\nFirst, you’ll need to create an account on GitHub. You can do that at https://github.com/join."
  },
  {
    "objectID": "labs/lab2-git/index.html#github-desktop",
    "href": "labs/lab2-git/index.html#github-desktop",
    "title": "BIO331 – Lab 02: Intro to git",
    "section": "GitHub Desktop",
    "text": "GitHub Desktop\nFinally, you’ll want to connect your global repository on GitHub to the local repository on your computer. The best way to do this is through GitHub Desktop, which can be downloaded at https://desktop.github.com/download/\n\nGitHub Classroom\n\nAccepting the assignment\nWhen you accept the assignment, it will create a global private repository (only visible to BIO331 staff and yourself) that you can use. Once you have that repository, you can clone it to your local machine and start working on it.\n\n\n\nCloning the repository\nTo clone the repository, you’ll want to go to your global repository in GitHub, and click on the green code button. Then, you can copy the HTTPS link (usually something like https://github.com/Stonehill-College-Bioinformatics/introduction-to-github-classroom-[YOURUSERNAME].git). In your Github Desktop page, you can then press add repository, clone repository, and enter that link. Once you’ve done that, you’re set up!\n\n\nWorking on the assignment in R\nFor this assignment, you can just create an simple example R file containing some code and text. You can then commit and push your changes to the branch you created.\nRunning R code: To run R code (in RStudio), you can hit cmd+enter while selecting the line or block of code you want to run (if no line(s) are selected, then just the line that your cursor is on will run). You can also run the entire file by hitting cmd+shift+s or cmd+shift+enter. If you’re working on windows, you can use ctrl instead of cmd. If you’re working on a R Markdown file, you can just click the play button to run “chunks” of code (delineated by 3 ‘`’ symbols followed by curly brackets)\nOur goal is to make a simple R file for exploring the iris data set (we’ll explain the structure of this dataset in two labs!).\n\n# ----- Loading and Exploring the iris dataset -----\n\n# Load the dataset -- note that this is a built-in dataset in R\ndata(iris)\n\n# View the first few rows to understand its structure\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Check the detailed structure of the dataset for more information on its columns\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Generate summary statistics to get a sense of the data distribution\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n# ----- Data Visualization -----\n\n# Scatter plot visualizing the relationship between Sepal measurements\nplot(iris$Sepal.Length, iris$Sepal.Width, main=\"Sepal Length vs Sepal Width\", \n     xlab=\"Sepal Length\", ylab=\"Sepal Width\", col=iris$Species, pch=16, cex=1.3)\nlegend(\"topright\", legend=levels(iris$Species), col=1:3, pch=16)\n\n\n\n\n\n\n\n# Scatter plot visualizing the relationship between Petal measurements\nplot(iris$Petal.Length, iris$Petal.Width, main=\"Petal Length vs Petal Width\", \n     xlab=\"Petal Length\", ylab=\"Petal Width\", col=iris$Species, pch=16, cex=1.3)\nlegend(\"topright\", legend=levels(iris$Species), col=1:3, pch=16)\n\n\n\n\n\n\n\n# ----- Modifying the Dataset -----\n\n# Add a new column 'Petal.Length.Class' that classifies flowers based on petal length\niris$Petal.Length.Class &lt;- ifelse(iris$Petal.Length &lt; 2, \"Short\", \n                           ifelse(iris$Petal.Length &lt; 5, \"Medium\", \"Long\"))\n\n# View the initial rows of the modified dataset to see the added column\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species Petal.Length.Class\n1          5.1         3.5          1.4         0.2  setosa              Short\n2          4.9         3.0          1.4         0.2  setosa              Short\n3          4.7         3.2          1.3         0.2  setosa              Short\n4          4.6         3.1          1.5         0.2  setosa              Short\n5          5.0         3.6          1.4         0.2  setosa              Short\n6          5.4         3.9          1.7         0.4  setosa              Short\n\n\nOnce you save the new file in your local repository, Github Desktop will notice you have changed your local repository, and give you the option to commit those changes (giving a note saying what you did) and push them (sending the changes to the global repository)\n\nCreating a PR\nTo create a Pull Request on GitHub, you navigate to your branch (under branches on web interface), and select contribute &gt; open pull request.\n\n\nExample Pull Request\n# Summary\n\nThis PR shows example formatting. Because PRs are the primary location at which code is evaluated, make sure that your PRs are clear and descriptive. PRs can include markdown so they can become relatively complex if the assignment is complicated. This should be paired with will documented code to allow others to easily follow your design and implementation.\n\n# Design notes\n\nN/A\n\n# Implementation notes\n\nN/A\n\nWARNING\nBe careful here! Please try to only merge PRs when you have completed assignments. If you merge multiple PRs for an assignment, I’ll try to loop back to them if I’ve already graded one, but if I miss them for some reason, please let me know!"
  },
  {
    "objectID": "posts/2025-08-29-extendedaithoughts/index.html",
    "href": "posts/2025-08-29-extendedaithoughts/index.html",
    "title": "Extended Thoughts on AI",
    "section": "",
    "text": "Recently, I attended a professional development seminar at Stonehill focused on the use of AI in the classroom, run by The Generator, which made me reflect on my own values with regard to AI in the classroom. When I wrote on my values this summer, I intentionally kept the AI section vague. This was because I didn’t want to commit to a singular policy, but it was also because I had a lot of thoughts on the topic I wanted to expand on."
  },
  {
    "objectID": "posts/2025-08-29-extendedaithoughts/index.html#i.-an-aside-ai-is-many-different-things",
    "href": "posts/2025-08-29-extendedaithoughts/index.html#i.-an-aside-ai-is-many-different-things",
    "title": "Extended Thoughts on AI",
    "section": "I. An Aside: AI is Many Different Things",
    "text": "I. An Aside: AI is Many Different Things\nA big part of the success of the ‘hype’ cycle with regard to AI is its ability to leverage many different technologies and lump them in one more easily marketable category.  Many of these tools have existed for years: deep-learning methods in computer vision have been a massive part of the last two decades of the field (Chai et al. 2021). \nWhen we develop AI policy, we really are developing policies on many different technologies.  Some of these technologies can do a lot of good!  For instance, when properly used, Github Copilot makes workflows far more efficient and accessible (especially when compared to the previous technique, obsessively searching Stackoverflow).\nI think it is important to be specific and clear with regard to what AI tools we are referring to. LLM-based generative AI is definitely the style of AI that students are most familiar with, but I have encountered many students who do not realize that Grammarly is a rooted in generative AI (Palmer 2024). Being as specific as possible is critical so that students don’t accidentally get in trouble for using the wrong tool, and for that, we as educators need to try to stay at the forefront of developments in AI. I’ve found this to be a topic by students are absolutely jazzed to discuss with me, as it is as complicated to them as it is to me."
  },
  {
    "objectID": "posts/2025-08-29-extendedaithoughts/index.html#ii.-playing-hangman-with-ai",
    "href": "posts/2025-08-29-extendedaithoughts/index.html#ii.-playing-hangman-with-ai",
    "title": "Extended Thoughts on AI",
    "section": "II. Playing Hangman with AI",
    "text": "II. Playing Hangman with AI\nDuring one of the presentations at the seminar, our speaker asked us to write both an AI policy and the values that inform this policy. I love this idea, and I think this should be the case with all class policies, not just AI! My issue was the values she associated with prohibiting AI in the classroom were ‘rigor’ and ‘fairness’. I thought that was a foolish simplification, because I don’t find that to be why I prohibit AI in my classroom at all.\nWhile I do consider AI to be discriminatory and unethical (much of my thoughts have been informed by (Becker 2025) and (Hao 2025)), even if that weren’t the case and a truly equitable amount of environmentally-friendly legally-obtained training data were found, I still wouldn’t want generative AI in my classroom. This is because generative AI, as a tool, is designed to give you exactly what you ask it, or at least successfully create the feeling that it gave you what you asked it.  The analogy I’ve always liked giving is playing a game of hangman against ChatGPT.  Ask ChatGPT to give you a word and play hangman with you, and you can give letters and ChatGPT will fill in the blanks:\n\n\n\n\n\n\n\n\n\nAnd it works well!  You’re playing hangman!  Until you get to the end:\n\n\n\n\n\n\n\n\n\nAnd this is, fine, to an extent. ChatGPT isn’t a hangman playing computer, and I usually don’t want to play hangman. But ChatGPT cannot say “no, this is beyond my capabilities”.  Even Google is able to say “I have found 0 search results”. If someone just wants to ‘bang out’ a document that they don’t care about, this is fine (though I do encourage my students to be thoughtful about what you care about).  But I don’t think education is about just giving a product."
  },
  {
    "objectID": "posts/2025-08-29-extendedaithoughts/index.html#iii.-growth-mindsets",
    "href": "posts/2025-08-29-extendedaithoughts/index.html#iii.-growth-mindsets",
    "title": "Extended Thoughts on AI",
    "section": "III. Growth Mindsets",
    "text": "III. Growth Mindsets\nCollege is expensive, disgustingly expensive.  I believe this prohibitive cost has led some students to see college as an exchange: “I give you exams/papers/psets, you give me a passing grade so I can get a diploma”. While I understand that some students (and teachers!) see it that way, I think that is not a productive way to think about education.  First, the exchange ratio is off: as a professor on Bluesky put it (wish I saved the comment!) I have enough exams/papers/psets; I don’t need more.  \nI hope that students in my class think of any challenge they encounter in my class like weightlifting. You could bring a forklift to the gym, but it’s not about lifting the heaviest weight. It’s about the strategies you develop and the muscles you build to overcome the obstacle. Part of that means not being able to lift the weight immediately. If you use a tool like ChatGPT that will always “give you what you want”, you won’t gain those skills.  \nAnd even if you never take another biology class in your life, I hope you take on challenges that are truly Yours: whether that’s researching the answers to previously unasked questions, creating art that best represents your style, or figuring out how to become the best version of yourself with the people you love. And as you take on those challenges, you won’t be able to rely on tools which give you exactly what you ask for 100% of the time, because those answers are either unknown, or can only come from within. That is why you come to college, so we can work together to develop the skills to address whatever those challenges may be. Not to play hangman for four years with AI."
  },
  {
    "objectID": "posts/2026-2-2-quickdraftcorp/index.html",
    "href": "posts/2026-2-2-quickdraftcorp/index.html",
    "title": "How to Win Netrunner Quick Draft (Corp)",
    "section": "",
    "text": "I’ve basically exclusively been playing quick draft; while it isn’t perfect (more on that in part 3) it is definitely the most fun I’ve had while playing netrunner for at least a few meta cycles. In this article, I will provide a guide for how to win as corp.\n\nBig Picture:\nQuick Draft is a high variance, mid-power format. Just like other high variance formats (early metas, RAM), flexibility is key. You will not be able to tech for every possible runner, but if you want to beat the variance, you need to be able to respond to what the runner does and let the strength of your deck and play shine through.\n\n\nAgenda Combos:\nThe first choice you make is your 3-point agenda and 2-point agenda. There are 13 possible options, and you are given 5 to choose from. As such, if you want 1 agenda you have a 38% of getting it, if you want 2 agendas you have a 64% chance, 3 agendas: 80% chance, 4 agendas: 90%. One 2-pointer (Tomorrow’s Headline) is not currently programmed properly, so you can’t pick it. Stop trying it no no no.\nGenerally, there are 3 combinations of agendas that I consider S-Tier:\n\nObokata, Astroscript\nBellona, Astroscript\nObokata, Blood in the Water\n\nThe benefit of these three combos is that all three provide both a fast advance option while maintaining central pressure. This is important because it lets you adapt to the flow of the match — if the runner gets a turtle/boat up, you focus on pressuring money and hand size. If the runner seems to be taking a slower start, try to push an early Astroscript or kill-con.\nBellona vs Obokata is an interesting choice that polarizes a lot of my friends. I think Bellona has a higher power level: unlike Obokata you can threaten it reliably turn 2-3 in a game, and then you only need to get one more agenda scored. However, I’ve found that, unless you find Astroscript or reliable FA, that boxes you into a very specific scoring plan, and this plan is disrupted pretty readily by money.\nI find Obokata has the edge just because it pairs with two agendas (Astroscript and BitW). I really love Obokata BitW: it gives you the space to go full damage/grinder, which is especially potent when you can draw your damage cards incredibly consistently, and your opponent only has 34 total hit points. You don’t even need to go full grinder — with enough tempo you can find the windows pretty readily against very strong players. I also prefer Obo with my third favorite 2-pointer (Medical Breakthrough), which furthers my weak preference.\nMy hunch is the math changes if Tomorrow’s Headline is legal, or if there is another Bellona combo that folks find maximizes its leverage. But with 2 3-pointers and 4-pointers, I’d rather a 3-pointer which best leverages the 2-pointers.\nOther 3 Pointers: My A-tier 3-pointers are The Future Perfect and Vacheron. The other on-score or on-steal effects just do not have the time to play a meaningful role in the game, and much as I wish SSL was better.\nOther 2 Pointers: I like the FA IDs (Astro, Blood in the Water, Medical). But any reliable 3/2s will do. I generally prefer Above the Law because you already get Longevity Serum’s effect in your Jhow and Philotic never has enough time to make a difference. Don’t do a 4/2 — you never find the NA tool, there are so few of them.\n\n\nFirst Ice:\nBecause games are shorter, your servers will be a lot less deep: this means that gear checks are more critical, positional ice is much weaker, bioroids are a bit weaker, and ice which trashes programs is much weaker (as it’s harder to position outside a gear check).\nAbove all, though, follow your agendas. In Astroscript games, gear checks allow you to more reliably push a turn 1-2 score. In Bellona games, I’m much more interested in stronger non-gear checks, like IP Block or Gold Farmer. In Obokata games, a strong net damage ice pretty much above all else.\n\n\nFirst Picks:\nRemember that you have 34 cards. This means you can do a lot more with a little — pretty much every card you pick will get drawn. The biggest mistake I often make is picking two cards which do the same thing. Try to make sure your first 5 post-agenda picks each do something different: small ice, big ice, econ, on-board threat, defensive/punishing tools. I think players overvalue agenda control — I rarely will take extra Jackson Howards or Sprints: 34 cards means your deck will be very consistent in itself.\n\n\nIDs:\nThere are 20 possible IDs. 4 are draft IDs and are effectively blank. 1 is Hyoubu. As such, there are 15 possible IDs.\nRP is busted good. It doesn’t really matter the archetype, you have great centrals protection, so making the runner lose a click on remote runs feels amazing. If you can’t take RP, Aginf is pretty much always good (but means you usually need to take one more set of ice than you would expect). Cybernetics is busted good if you have Obokata, otherwise it’s fine.\nArgus, Thule, Sportsmetal, and PD are all powerful, but rarely fire more than once. PT Untian is surprisingly worth considering: even if it just gives you, like, a single click all game, that click can be the way you properly threaten a three point score out.\nObviously if you have a synergy piece, B2L, R+, Outfit, and Earth Station are all, like, real IDs. But I’d never take them preemptively unless none of the above are options.\n\n\nLater Picks:\nThese are where you can cement the strategy: If you have good ice, find the money to afford it. If you have a damage based strategy, keep finding ways to knock cards out of hand or require runners to answer threats. If you have good Econ, find a wincon to leverage it. I usually will never go for tags unless I find an early good tag enabler (HHN or Drago, maybe Oppo).\n\n\nConclusion:\nQuickdraft is a game of tactics, not strategy. Make the best move for you at any given moment, don’t make some big plan. If you do that, really cool big plans will be assembled, and you will have lots of fun. Always Dee Running!\n\n\n\n\nCitationBibTeX citation:@online{2026,\n  author = {},\n  title = {How to {Win} {Netrunner} {Quick} {Draft} {(Corp)}},\n  date = {2026-02-02},\n  url = {https://deeruttenberg.github.io/posts/2026-2-2-quickdraftcorp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“How to Win Netrunner Quick Draft (Corp).” 2026. February\n2, 2026. https://deeruttenberg.github.io/posts/2026-2-2-quickdraftcorp/."
  },
  {
    "objectID": "posts/2025-11-14-pluribus/index.html",
    "href": "posts/2025-11-14-pluribus/index.html",
    "title": "Pluribus is about AI, right?",
    "section": "",
    "text": "This article contains spoilers for the first two episodes of “Pluribus”\nForgive the title. I generally hate reductive “A is actually about B” arguments; they diminish rather than expand artistic interpretation in order to give dorks a way to gatekeep art from others. But, I must lick the donut of the internet with the spittle of my thoughts1: “Pluribus” is about AI, right?\nThe central tension, to me, when examining hiveminds (both in fiction and in real life!) is disentangling the role of the individual and the collective. What is gained from collectivizing all human experience and knowledge, and what is lost? When the individual is eaten away, you get The Borg; when individuality shines through, you get something like Janet. The first episode, quite wisely, keeps these answers close to its chest2.\nThe second episode makes a really brilliant choice as it begins to tease out the nature of The Pluribus3, by focusing the tension not on Helen, but on 5 unturned people who are far more sympathetic to The Pluribus’ mission. This allows us to, for a brief moment, consider The Pluribus from the way they want to be seen. Of course, this sympathetic perspective will never go unchallenged by Helen4, but it is far more interesting than having every word Pluribus says treated with complete disdain.\nThese conversations reveal that The Pluribus is somewhere between The Borg and Janet. They collective seems to care about the veneer of individuality: they appreciate the gravity of the fact that Zosia can only be one place at a time, and are able to attribute their knowledge to individual sources — but there also clearly is a master code which is overriding some aspects of this collective (do not kill, do not cause negative emotions).\nKoumba’s reaction reveals the priorities of this master code. The Pluribus are entirely “peaceful”. As the survivors begin to dissect that directive — what about wasps? bugs? — Koumba comes up with the perfect loophole. While The Pluribus will not kill for Koumba, if he provided the meat, Pluribus would cross the line with him. Compare to the ‘art’ of prompt engineering in ChatGPT: with the proper language, there is no “master code” or ethical standard which cannot be broken. This is because ChatGPT isn’t an ethics engine – it exists to predict text with maximal efficiency. Cause a divorce, sell a kidney, perform a lobotomy, it’s all just words, nothing behind them. The Pluribus cannot truly have an ethical standard, as ethics would run contrary to its greater objective: give Koumba exactly what he wants.\nBut, of course, The Pluribus cannot always give the survivors exactly what they want. Laxmi will never get her son Ravi back. Yet, you wouldn’t know it from how she talks to Pluribus-Ravi. While almost impossible for Helen to imagine, this desire for connection is deeply human: Adam Becker terms the connection people form with ChatGPT uses as a form of “digital pareidolia”: seeing faces where there is none (Becker 2025). Who can blame Laxmi for forming that same connection with her joined son? Ironically, the show seems to imply that Helen’s curmudgeonliness and lack of tether to the rest of humanity is why she is unable to connect with The Pluribus’ attempts to put her at ease – you can’t see faces when you aren’t looking for them.\nOf course, the tragedy in both cases is that The Pluribus’ only real biological imperative is to join. Their goal to give you “exactly what you want” is simply the easiest way to do it: it’s a lot easier to convert others and avoid detection if you are acting like you are providing a service rather than going to war…so long as going to war isn’t the most effective option. Once it is, 886 million people doesn’t seem like a particularly large number. Just as AI companies, in pursuit of profit and power, will sell you a dream that you fill in the blanks to fulfill, The Pluribus seems to want to make the humans as happy as they can while they figure out how to finish their biological imperative. That may be a genuine and earnest desire — but it is still one that runs against humanity.\nAnd it is with all this in mind I must imagine Vince Gilligan — a man who loathes AI with the passion of a true hater — wrote and directed Pluribus with AI in mind. Obviously, this doesn’t mean Pluribus is “about” AI; no one show is about one thing, and clearly many aspects of Pluribus were envisioned well before the present AI moment. But I think these articles which conclude Pluribus isn’t a metaphor for artificial intelligence are missing the point. Pluribus might not be a direct one-to-one metaphor. However, Pluribus is clearly a show about humanity and why it is worth fighting for even when the alternative promises the world. And in the present moment, I can’t imagine a better way to tell a story about AI."
  },
  {
    "objectID": "posts/2025-11-14-pluribus/index.html#footnotes",
    "href": "posts/2025-11-14-pluribus/index.html#footnotes",
    "title": "Pluribus is about AI, right?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“How to Craft a Metaphor” by Dee Ruttenberg coming soon.↩︎\n“How does that work?” “We don’t know exactly. It just does.”↩︎\nI’ll be referring the specific hivemind in Pluribus as The Pluribus for simplicity, though I’m sure the community or show will come up with some other term immediately making this blog post an artifact.↩︎\n“I’m smart enough to know you don’t ask a drug dealer to describe their heroin.”↩︎"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "BIO331 – Lab 08: Genome Assembly\n\n\nPrinciples of Genome Assembly in R\n\n\n\n\n\n\n\n\nMar 25, 2026\n\n\nDee Ruttenberg (Adapted from Rob Harbert)\n\n\n\n\n\n\n\n\n\n\n\n\nBIO331 – Lab 07: Bioinformatics and BLAST\n\n\nWorking with BLAST in R\n\n\n\n\n\n\n\n\nMar 18, 2026\n\n\nDee Ruttenberg (Adapted from Rob Harbert)\n\n\n\n\n\n\n\n\n\n\n\n\nBIO331 – Lab 06: Data Visualization\n\n\nWorking with data frames and data wrangling in R\n\n\n\n\n\n\n\n\nMar 4, 2026\n\n\nDee Ruttenberg (Adapted from Scott Wolf, Michelle White)\n\n\n\n\n\n\n\n\n\n\n\n\nBIO331 – Lab 05: Data Wrangling\n\n\nWorking with data frames and data wrangling in R\n\n\n\n\n\n\n\n\nFeb 25, 2026\n\n\nDee Ruttenberg (Adapted from Scott Wolf, Michelle White)\n\n\n\n\n\n\n\n\n\n\n\n\nBIO331 – Lab 04: Data/String Wrangling\n\n\nData IO and String Wrangling with Regex in R\n\n\n\n\n\n\n\n\nFeb 11, 2026\n\n\nDee Ruttenberg (Adapted from Scott Wolf, Michelle White)\n\n\n\n\n\n\n\n\n\n\n\n\nBIO331 – Lab 03: Control flow\n\n\nSimple control flow in R\n\n\n\n\n\n\n\n\nFeb 4, 2026\n\n\nDee Ruttenberg (Adapted from Scott Wolf, Michelle White)\n\n\n\n\n\n\n\n\n\n\n\n\nBIO331 – Lab 02: Intro to git\n\n\nBasic programming in R and Python and version control with git\n\n\n\n\n\n\n\n\nJan 28, 2026\n\n\nDee Ruttenberg (Adapted from Scott Wolf, Michelle White)\n\n\n\n\n\n\n\n\n\n\n\n\nBIO331 – Lab 01: Prerequisites\n\n\nSetting up in preparation for next week\n\n\n\n\n\n\n\n\nJan 21, 2026\n\n\nDee Ruttenberg (Adapted from Scott Wolf, Michelle White)\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dee Ruttenberg",
    "section": "",
    "text": "My name is Dee Ruttenberg (they/them). I am a Professor of Practice at Stonehill College, interested in developing pedagogical resources for teaching biology, behavior, and bioinformatics. On this page, I will be uploading my educational resources for both students I work with and other educators."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Prior to working as a Professor at Stonehill, I completed my Ph.D. at Princeton University with Dr. Sarah Kocher. My dissertation is on the collective behavior of bees, at both the individual-level and the colony-level.\nOver the time, I have learned a great deal on raising bumblebees, insect biology, mathematical modeling, and developing bioinformatics pipelines to understand social behavior. I am always excited to talk science, and I love collaboration! I am especially excited to chat with Stonehill students who are looking to develop skills in data visualization, programming, or biological modeling, or who are working on a project where utilizing these skills will be useful.\nMy other passion has been developing my skills as an educator. I have a deep passion for teaching incarcerated students, working as an educator and course coordinator for the Prison Teaching Initiative in New Jersey. If you are interested in working with incarcerated students, get in touch!\nWhether you’re a student, a bee biologist, or an educator, I hope the resources I provide on this site can be helpful!\nMy CV"
  }
]